import csv
import os
import glob
import nltk
from nltk import FreqDist
from nltk.tokenize import sent_tokenize, word_tokenize
from nltk.corpus import stopwords
import itertools

tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')

adwords_filepath, all_comments_filepath = assign_location_dirs()
locations = make_locations_list_from_adwords(adwords_filepath)
meth_words, sub_words, nalt_words, narc_words = make_mat_list()
total_comment_list = all_comment_text(all_comments_filepath)



def assign_working_dirs(thread_folder_name, comment_folder_name, comment_working_folder_name, comment_master_folder_name, comment_duplicate_folder_name, comment_complete_folder_name, subreddit, file_folder=None):
    """
    Assigns working directories according to which computer being run on
    """
    if file_folder:
        use_path = os.path.join(file_folder, subreddit)
    else:
        if os.getcwd().split('/')[2] == 'akilby':
            use_path = "/Users/akilby/Dropbox/Research/Data/drug_pricing_data/%s/" % subreddit
        else:
            use_path = "/Users/jackiereimer/Dropbox/drug_pricing_data/%s/" % subreddit
    thread_folder = os.path.join(use_path, thread_folder_name)
    comment_folder = os.path.join(use_path, comment_folder_name)
    comment_working_folder = os.path.join(comment_folder, comment_working_folder_name)
    comment_duplicate_folder = os.path.join(comment_folder, comment_duplicate_folder_name)
    comment_master_folder = os.path.join(comment_folder, comment_master_folder_name)
    comment_complete_folder = os.path.join(comment_folder, comment_complete_folder_name)
    print('Thread folder: %s' % thread_folder)
    print('Comment folder: %s' % comment_folder)
    print('Master folder: %s' % comment_master_folder)
    print('Duplicate folder: %s' % comment_duplicate_folder)
    print('Working folder: %s' % comment_working_folder)
    print('Complete folder: %s' % comment_complete_folder)
    return thread_folder, comment_folder, comment_working_folder, comment_master_folder, comment_duplicate_folder, comment_complete_folder


def assign_location_dirs():
    if os.getcwd().split('/')[2] == 'akilby':
        adwords_filepath = '/Users/akilby/Dropbox/Drug Pricing Project/locations/AdWords.csv'
        all_comments_filepath = '/Users/akilby/Dropbox/Research/Data/drug_pricing_data/opiates/comments/complete/all_comments.csv'
    else:
        adwords_filepath = '/Users/jackiereimer/Dropbox/Drug Pricing Project/locations/Adwords.csv'
        all_comments_filepath = '/Users/jackiereimer/Dropbox/drug_pricing_data/opiates/comments/complete/all_comments.csv'
    return adwords_filepath, all_comments_filepath


def make_locations_list_from_adwords(adwords_filepath):
    with open(adwords_filepath, 'r') as in_file:
        location_file = csv.reader(in_file)
        locations = list(location_file)
    locations = [x[:5] for x in locations]
    locations = [x[:2] + x[2].split(',') + x[3:] for x in locations]
    locations = list(itertools.chain.from_iterable(locations))
    locations = list(set(locations))
    return locations


def make_mat_list():
    meth_words = ['methadone', 'mmt']
    sub_words = ['suboxone', 'sub', 'suboxone', 'buprenex', 'butrans', 'probuphine', 'belbuca', 'bupe']
    nalt_words = ['naltrexone' 'reviva', 'vivitrol', 'uldn']
    narc_words = ['naloxone', 'narcan']
    meth_words = [x.split(',') for x in meth_words]
    sub_words = [x.split(',') for x in sub_words]
    nalt_words = [x.split(',') for x in nalt_words]
    narc_words = [x.split(',') for x in narc_words]
    meth_words = list(itertools.chain.from_iterable(meth_words))
    sub_words = list(itertools.chain.from_iterable(sub_words))
    nalt_words = list(itertools.chain.from_iterable(nalt_words))
    narc_words = list(itertools.chain.from_iterable(narc_words))
    meth_words = list(set(meth_words))
    sub_words = list(set(sub_words))
    nalt_words = list(set(nalt_words))
    narc_words = list(set(narc_words))
    return meth_words, sub_words, nalt_words, narc_words


all_comments_filepath = '/Users/jackiereimer/Documents/duplicates/opiates_7tww8l-1517606138.csv'

def all_comment_text(all_comments_filepath):
    total_comment = []
    with open(all_comments_filepath, 'r') as f:
        reader = csv.reader(f)
        for row in reader:
            total_comment.append(row[4])
    total_comment_list = list(set(total_comment))
    return total_comment_list



### THIS IS VERSION 1 ####
def identify_location_sentences(all_comments_filepath, total_comment_list, locations):
    with open(all_comments_filepath, 'r') as read_file:
        reader = csv.reader(read_file)
        unique_location_comments = []
        for comment in reader:
            location_comment = [comment for comment in total_comment_list if location in comment]
            location_comment2 = location_comment[:2] + location_comment[3:]
            if location_comment2 not in unique_location_comments:
                unique_location_comments.append(location_comment)
            print('added %s' % location_comment)
    outfilename = '/Users/jackiereimer/Desktop/test2/test.csv'
    with open(outfilename, 'w') as outfile:
        print(outfilename)
        writer = csv.writer(outfile)
        writer.writerows(unique_location_comments)



def identify_key_word_comments(all_comments_filepath, total_comment_list, list):
	''' looks for terms within a set across all comments within a given filepath'''
    target_comments = []
    with open(all_comments_filepath, 'r') as read_file:
    	reader = csv.reader(read_file)
    	for comment in reader:



def identify_location_sentences(all_comments_filepath, total_comment_list, locations):
    unique_location_comments = []
    for location in locations:
        location_comment = [comment for comment in total_comment_list if location in comment]
        if location_comment not in unique_location_comments:
            unique_location_comments.append(location_comment)
        print('location: %s; location comment %s' % (location, location_comment))
    outfilename = '/Users/jackiereimer/Desktop/test2/test.csv'
    with open(outfilename, 'w') as outfile:
         print(outfilename)
         writer = csv.writer(outfile)
         writer.writerows(unique_location_comments)





for locations in locations:
    location_comment = [comment for comment in total_comment_list if location in comment]
    if location_comment == '':
        pass
    else:
        print(location, location_comment)




def mat_fdist(all_comments_filepath, mat_words)
    stop_words = set(stopwords.words("english"))
    for comment in total_comment_list:
    	fdist = FreqDist(w for w in all_words_set if not w in mat_words or stop_words)
    print(fdist)















    location_mention = []
    for comment in total_comment_list:
        total_sentences = sent_tokenize(comment)
        for sentence in total_sentences:
            total_words = word_tokenize(sentence)
            for word in total_words:
                if word in locations:
                    location_mention.append(word)
    locations_mentioned = set(location_mention)
    return locations_mentioned
    location_comments = [comment for comment in total_sentences if locations_mentioned in sent]
    return location_sentences



def identify_mat_sentences(total_comment_list, mat_terms):
    mat_mention = []
    for comment in total_comment_list:
        total_sentences = sent_tokenize(comment)
        for sentence in total_sentences:
            total_words = word_tokenize(sentence)
            for word in total_words:
                if word in mat_terms:
                    mat_mention.append(word)
    mat_mentioned = set(mat_mention)
    mat_sentences = [sent for sent in total_sentences if mat_mentioned in sent]
    return location_sentences



    total_sentence_list.append(total_sentences)
    print(filttered_word_list)


def remove_stopwords(all_comments_filepath):
    stop_words = set(stopwords.words("english"))
    for comment in total_comment_list:
        total_sentences = sent_tokenize(total_comment_list)
        all_words = []
        for sentence in total_sentences:
            sent_words = word_tokenize(sentence)
            relevant_words = [w for w in sent_words if not w in stop_words]
            all_words.append(relevant_words)
    all_words_set = set(all_words)
    print(all_words_set)

def mat_fdist(all_words_set, mat_words)
    stop_words = set(stopwords.words("english"))
    for comment in total_comment_list:
    	fdist = FreqDist(w for w in all_words_set if not w in mat_words)
    print(fdist)







def all_submissions(thread_folder):
    row_list = []
    for dumpname in os.listdir(thread_folder):
        filepath_use = os.path.join(thread_folder, dumpname)
        if not dumpname.startswith('.'):
            with open(filepath_use, 'r') as f:
                reader = csv.reader(f)
                for row in reader:
                    a = tuple(row)
                    row_list.append(a)
    row_list = list(set(row_list))
    all_submissions_filepath = os.path.join(thread_folder, 'all_dumps.csv')
    print(all_submissions_filepath)
    with open(all_submissions_filepath, 'w') as f:
        writer = csv.writer(f)
        for row in row_list:
            a = list(row)
            writer.writerow(a)


def all_comment_body(comment_complete_folder):
    row_list = []
    for commentfile in os.listdir(comment_complete_folder):
        filepath_use = os.path.join(comment_complete_folder, commentfile)
        if not commentfile.startswith('.'):
            with open(filepath_use, 'r') as f:
                reader = csv.reader(f)
                for row in reader:
                    a = tuple(row)
                    row_list.append(a)
    row_list = list(set(row_list))
    all_comments_filepath = os.path.join(comment_complete_folder, 'all_comments.csv')
    with open(all_comments_filepath, 'w') as f:
        writer = csv.writer(f)
        for row in row_list:
            a = list(row)
            writer.writerow(a)



def find_locations(file, locations)
    location_sentences = []
    related_sentences = []
    post_sentence_list = sent_tokenize(complete_file)
        for sentence in post_sentence_list:
            comment_word_list = word_tokenize(sentence) 
            if location in comment_word_list:
                return sentence
                location_sentences.append(sentence)
                try:
                    return post_sentence_list[sentence-1]
                    related_sentences.appen(sentence-1)
                except IndexError as e:
                    print('Top level post')
                    pass
                try:
                    return post_sentence_list[sentence+1]
                except IndexError as e:
                    print('Last comment in post')
                    pass




meth_words = ['methadone', 'mmt']
sub_words = ['suboxone', 'sub', 'suboxone', 'buprenex', 'butrans', 'probuphine', 'belbuca', 'bupe']
nalt_words = ['naltrexone' 'reviva', 'vivitrol', 'uldn']
narc_words = ['naloxone, narcan']


import nltk
file = open('/Users/jackiereimer/Desktop/test2/opiates_7x1k1c-1518663169 copy.csv').read()
tokenized = nltk.sent_tokenize(file)
is_mat = [sent for sent in tokenized if ]






from nltk import FreqDist
import re

raw = open('/Users/jackiereimer/Desktop/test2/opiates_7x1k1c-1518663169 copy.csv').read()
fdist = nltk.FreqDist(wd.lower() for wd in raw if )



f = open('/Users/jackiereimer/Desktop/test2/opiates_7x1k1c-1518663169 copy.csv')
for line in f:
    print(line.strip())

raw = open('/Users/jackiereimer/Desktop/test2/opiates_7x1k1c-1518663169 copy.csv').read()
type(raw)


tokens = word_tokenize(raw)
type(tokens)

words = [w.lower() for w in tokens]
type(words)

vocab = sorted(set(words))
type(vocab)



